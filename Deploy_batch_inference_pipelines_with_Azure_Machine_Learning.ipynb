{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Register a modelt**"
      ],
      "metadata": {
        "id": "VH4ajUF-WTVK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cGyTzBcV-82"
      },
      "outputs": [],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "classification_model = Model.register(workspace=your_workspace,\n",
        "                                      model_name='classification_model',\n",
        "                                      model_path='model.pkl', # local path\n",
        "                                      description='A classification model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.register_model( model_name='classification_model',\n",
        "                    model_path='outputs/model.pkl', # run outputs path\n",
        "                    description='A classification model')"
      ],
      "metadata": {
        "id": "4oAQ7hCTWX2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Create a scoring script**"
      ],
      "metadata": {
        "id": "1TQxAdfQWZqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from azureml.core import Model\n",
        "import joblib\n",
        "\n",
        "def init():\n",
        "    # Runs when the pipeline step is initialized\n",
        "    global model\n",
        "\n",
        "    # load the model\n",
        "    model_path = Model.get_model_path('classification_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "def run(mini_batch):\n",
        "    # This runs for each batch\n",
        "    resultList = []\n",
        "\n",
        "    # process each file in the batch\n",
        "    for f in mini_batch:\n",
        "        # Read comma-delimited data into an array\n",
        "        data = np.genfromtxt(f, delimiter=',')\n",
        "        # Reshape into a 2-dimensional array for model input\n",
        "        prediction = model.predict(data.reshape(1, -1))\n",
        "        # Append prediction to results\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
        "    return resultList"
      ],
      "metadata": {
        "id": "CIK--6XeWX56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Create a pipeline with a ParallelRunStep**"
      ],
      "metadata": {
        "id": "88YIPknXX0Uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "# Get the batch dataset for input\n",
        "batch_data_set = ws.datasets['batch-data']\n",
        "\n",
        "# Set the output location\n",
        "output_dir = OutputFileDatasetConfig(name='inferences')\n",
        "\n",
        "# Define the parallel run step step configuration\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory='batch_scripts',\n",
        "    entry_script=\"batch_scoring_script.py\",\n",
        "    mini_batch_size=\"5\",\n",
        "    error_threshold=10,\n",
        "    output_action=\"append_row\",\n",
        "    environment=batch_env,\n",
        "    compute_target=aml_cluster,\n",
        "    node_count=4)\n",
        "\n",
        "# Create the parallel run step\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='batch-score',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
        "    output=output_dir,\n",
        "    arguments=[],\n",
        "    allow_reuse=True\n",
        ")\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
      ],
      "metadata": {
        "id": "XM8JwUQxX22Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Run the pipeline and retrieve the step output**"
      ],
      "metadata": {
        "id": "Xu1Eq2LvX4QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "# Run the pipeline as an experiment\n",
        "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
        "pipeline_run.wait_for_completion(show_output=True)\n",
        "\n",
        "# Get the outputs from the first (and only) step\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='results')\n",
        "\n",
        "# Find the parallel_run_step.txt file\n",
        "for root, dirs, files in os.walk('results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# Load and display the results\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "print(df)"
      ],
      "metadata": {
        "id": "dj2DKz4XX6-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Publishing a batch inference pipeline**"
      ],
      "metadata": {
        "id": "GtYCF6kDYAu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
        "                                                   description='Batch pipeline',\n",
        "                                                   version='1.0')\n",
        "rest_endpoint = published_pipeline.endpoint"
      ],
      "metadata": {
        "id": "sw6qyid1X97P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(rest_endpoint,\n",
        "                         headers=auth_header,\n",
        "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
        "run_id = response.json()[\"Id\"]"
      ],
      "metadata": {
        "id": "jiWZh4BwYDgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
        "\n",
        "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
        "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
        "                                        description='batch inferencing',\n",
        "                                        pipeline_id=published_pipeline.id,\n",
        "                                        experiment_name='Batch_Prediction',\n",
        "                                        recurrence=weekly)"
      ],
      "metadata": {
        "id": "y3VjowO9YDj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}