{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline steps**"
      ],
      "metadata": {
        "id": "Jsfd4BUpDJMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An Azure Machine Learning pipeline consists of one or more steps that perform tasks. There are many kinds of steps supported by Azure Machine Learning pipelines, each with its own specialized purpose and configuration options.\n",
        "\n",
        "Common kinds of step in an Azure Machine Learning pipeline include:\n",
        "\n",
        "PythonScriptStep: Runs a specified Python script.\n",
        "DataTransferStep: Uses Azure Data Factory to copy data between data stores.\n",
        "DatabricksStep: Runs a notebook, script, or compiled JAR on a databricks cluster.\n",
        "AdlaStep: Runs a U-SQL job in Azure Data Lake Analytics.\n",
        "ParallelRunStep - Runs a Python script as a distributed task on multiple compute nodes."
      ],
      "metadata": {
        "id": "L-MRXprxDPUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF58ia7lDB5R"
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "# Step to run a Python script\n",
        "step1 = PythonScriptStep(name = 'prepare data',\n",
        "                         source_directory = 'scripts',\n",
        "                         script_name = 'data_prep.py',\n",
        "                         compute_target = 'aml-cluster')\n",
        "\n",
        "# Step to train a model\n",
        "step2 = PythonScriptStep(name = 'train model',\n",
        "                         source_directory = 'scripts',\n",
        "                         script_name = 'train_model.py',\n",
        "                         compute_target = 'aml-cluster')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining the steps, you can assign them to a pipeline, and run it as an experiment:"
      ],
      "metadata": {
        "id": "xlM2mWaADVSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core import Experiment\n",
        "\n",
        "# Construct the pipeline\n",
        "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2])\n",
        "\n",
        "# Create an experiment and run the pipeline\n",
        "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
        "pipeline_run = experiment.submit(train_pipeline)"
      ],
      "metadata": {
        "id": "qxa5t1V6DTM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass data between pipeline steps"
      ],
      "metadata": {
        "id": "rei9J37EDZ0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OutputFileDatasetConfig Object"
      ],
      "metadata": {
        "id": "JYQ-elxgDbqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OutputFileDatasetConfig Step Inputs and Outputs"
      ],
      "metadata": {
        "id": "99-leUbSDhxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
        "\n",
        "# Get a dataset for the initial data\n",
        "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
        "\n",
        "# Define a PipelineData object to pass data between steps\n",
        "data_store = ws.get_default_datastore()\n",
        "prepped_data = OutputFileDatasetConfig('prepped')\n",
        "\n",
        "# Step to run a Python script\n",
        "step1 = PythonScriptStep(name = 'prepare data',\n",
        "                         source_directory = 'scripts',\n",
        "                         script_name = 'data_prep.py',\n",
        "                         compute_target = 'aml-cluster',\n",
        "                         # Script arguments include PipelineData\n",
        "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\n",
        "                                      '--out_folder', prepped_data])\n",
        "\n",
        "# Step to run an estimator\n",
        "step2 = PythonScriptStep(name = 'train model',\n",
        "                         source_directory = 'scripts',\n",
        "                         script_name = 'train_model.py',\n",
        "                         compute_target = 'aml-cluster',\n",
        "                         # Pass as script argument\n",
        "                         arguments=['--training-data', prepped_data.as_input()])"
      ],
      "metadata": {
        "id": "0vTTES-eDi4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code in data_prep.py\n",
        "from azureml.core import Run\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# Get arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--raw-ds', type=str, dest='raw_dataset_id')\n",
        "parser.add_argument('--out_folder', type=str, dest='folder')\n",
        "args = parser.parse_args()\n",
        "output_folder = args.folder\n",
        "\n",
        "# Get input dataset as dataframe\n",
        "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
        "\n",
        "# code to prep data (in this case, just select specific columns)\n",
        "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
        "\n",
        "# Save prepped data to the PipelineData location\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
        "prepped_df.to_csv(output_path)"
      ],
      "metadata": {
        "id": "C9ZcxyLSDkYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reuse pipeline steps**"
      ],
      "metadata": {
        "id": "ZsGpb1N_DpDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Managing step output reuse\n",
        "By default, the step output from a previous pipeline run is reused without rerunning the step provided the script, source directory, and other parameters for the step haven't changed. Step reuse can reduce the time it takes to run a pipeline, but it can lead to stale results when changes to downstream data sources haven't been accounted for."
      ],
      "metadata": {
        "id": "WwF9exD7DsZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = PythonScriptStep(name = 'prepare data',\n",
        "                         source_directory = 'scripts',\n",
        "                         script_name = 'data_prep.py',\n",
        "                         compute_target = 'aml-cluster',\n",
        "                         runconfig = run_config,\n",
        "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
        "                         outputs=[prepped_data],\n",
        "                         arguments = ['--folder', prepped_data]),\n",
        "                         # Disable step reuse\n",
        "                         allow_reuse = False)"
      ],
      "metadata": {
        "id": "4o9rKmO2DvNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Publish pipelines**"
      ],
      "metadata": {
        "id": "SvorBZDEDyEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline.publish(name='training_pipeline',\n",
        "                                          description='Model training pipeline',\n",
        "                                          version='1.0')"
      ],
      "metadata": {
        "id": "U1uaCteIDwNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most recent run of the pipeline\n",
        "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
        "run = list(pipeline_experiment.get_runs())[0]\n",
        "\n",
        "# Publish the pipeline from the run\n",
        "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
        "                                          description='Model training pipeline',\n",
        "                                          version='1.0')"
      ],
      "metadata": {
        "id": "kHH8TsgTD2WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a published pipeline**"
      ],
      "metadata": {
        "id": "EJolKsT1D5n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(rest_endpoint,\n",
        "                         headers=auth_header,\n",
        "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "print(run_id)"
      ],
      "metadata": {
        "id": "MLB4gmwnD3os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use pipeline parameters**"
      ],
      "metadata": {
        "id": "wz1XM00ED-If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "\n",
        "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
        "\n",
        "...\n",
        "\n",
        "step2 = PythonScriptStep(name = 'train model',\n",
        "                         source_directory = 'scripts',\n",
        "                         script_name = 'train_model.py',\n",
        "                         compute_target = 'aml-cluster',\n",
        "                         # Pass parameter as script argument\n",
        "                         arguments=['--in_folder', prepped_data,\n",
        "                                    '--reg', reg_param],\n",
        "                         inputs=[prepped_data])"
      ],
      "metadata": {
        "id": "KCkhQhjJD_5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running a pipeline with a parameter**"
      ],
      "metadata": {
        "id": "K3SPqG5hEDlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(rest_endpoint,\n",
        "                         headers=auth_header,\n",
        "                         json={\"ExperimentName\": \"run_training_pipeline\",\n",
        "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})"
      ],
      "metadata": {
        "id": "eVTeNCFiEDX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schedule pipelines"
      ],
      "metadata": {
        "id": "BD4CxAF1EMKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scheduling a pipeline for periodic intervals**"
      ],
      "metadata": {
        "id": "csF6OKSpEQC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
        "\n",
        "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
        "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
        "                                        description='trains model every day',\n",
        "                                        pipeline_id=published_pipeline.id,\n",
        "                                        experiment_name='Training_Pipeline',\n",
        "                                        recurrence=daily)"
      ],
      "metadata": {
        "id": "4GSUUFcjESdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Triggering a pipeline run on data changes**"
      ],
      "metadata": {
        "id": "O7-G12LDEVQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore\n",
        "from azureml.pipeline.core import Schedule\n",
        "\n",
        "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
        "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
        "                                    description='trains model on data change',\n",
        "                                    pipeline_id=published_pipeline.id,\n",
        "                                    experiment_name='Training_Pipeline',\n",
        "                                    datastore=training_datastore,\n",
        "                                    path_on_datastore='data/training')"
      ],
      "metadata": {
        "id": "KVGA0G7gEUk_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}